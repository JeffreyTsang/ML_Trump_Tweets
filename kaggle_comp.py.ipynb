{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsang\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#Import all the stuff\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import math\n",
    "import datetime\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#Import file\n",
    "file = r'.\\all\\train.csv'\n",
    "d = pd.read_csv(file)\n",
    "test_file = r'.\\all\\test.csv'\n",
    "test_d = pd.read_csv(test_file)\n",
    "\n",
    "data = d[[\"text\", \"created\", \"label\", \"retweetCount\"]] #Full dataset\n",
    "test_data = test_d[[\"text\", \"created\", \"retweetCount\"]]\n",
    "\n",
    "# print (data)\n",
    "stopWrds = stopwords.words('english')\n",
    "\n",
    "def formatTraining(x, whichFeat):\n",
    "    spl = x.split(\"/\")\n",
    "    if len(spl[0]) == 1:\n",
    "        spl[0] = \"0\" + spl[0]\n",
    "    if len(spl[1]) == 1:\n",
    "        spl[1] = \"0\" + spl[1]\n",
    "    x_new = \"/\".join(spl)\n",
    "    if whichFeat == \"weekday\":\n",
    "        return datetime.datetime.strptime(x_new, '%m/%d/%y %H:%M').weekday()\n",
    "    elif whichFeat == \"hour\":\n",
    "        return datetime.datetime.strptime(x_new, '%m/%d/%y %H:%M').hour\n",
    "     \n",
    "\n",
    "def formatTest(x, whichFeat):\n",
    "    spl = x.split(\"/\")\n",
    "    if len(spl[0]) == 1:\n",
    "        spl[0] = \"0\" + spl[0]\n",
    "    if len(spl[1]) == 1:\n",
    "        spl[1] = \"0\" + spl[1]\n",
    "    x_new = \"/\".join(spl)\n",
    "    if whichFeat == \"weekday\":\n",
    "        return datetime.datetime.strptime(x_new, '%m/%d/%Y %H:%M').weekday()\n",
    "    elif whichFeat == \"hour\":\n",
    "        return datetime.datetime.strptime(x_new, '%m/%d/%Y %H:%M').hour\n",
    "\n",
    "def formatTimes(x, time):\n",
    "    return 1 if time == x else 0\n",
    "\n",
    "def removeStops(x):\n",
    "    spl = x.split()\n",
    "    spl = [item for item in spl if item.lower() not in stopWrds]\n",
    "    return ' '.join(spl)\n",
    "\n",
    "def removeLinks(x):\n",
    "    if \"https\" in x:\n",
    "        x = x.split(\"https\")\n",
    "        return x[0] + \"https\"\n",
    "    return x\n",
    "\n",
    "fmtTime = np.vectorize(formatTimes)\n",
    "rmvStops = np.vectorize(removeStops)\n",
    "rmvLinks = np.vectorize(removeLinks)\n",
    "    \n",
    "vals = data[\"created\"].values\n",
    "fmtTrain = np.vectorize(formatTraining)\n",
    "day_created_single = fmtTrain(vals, \"weekday\")\n",
    "hour_created_single = fmtTrain(vals, \"hour\")\n",
    "data.loc[:, \"text\"] = data[\"text\"].apply(rmvStops)\n",
    "data.loc[:, \"text\"] = data[\"text\"].apply(rmvLinks)\n",
    "\n",
    "#create binary matrix for weekdays for training\n",
    "n = day_created_single.shape[0]\n",
    "\n",
    "day_created = np.zeros((n, 7))\n",
    "for i in range(7):\n",
    "    day_created[:,i] = fmtTime(day_created_single, i)\n",
    "\n",
    "hour_created = np.zeros((n, 24))\n",
    "for i in range(24):\n",
    "    hour_created[:, i] = fmtTime(hour_created_single, i)\n",
    "\n",
    "# test_data.loc[:, \"text\"] = test_data[\"text\"].apply(getHTTP)\n",
    "fmtTest = np.vectorize(formatTest)\n",
    "day_test_single = fmtTest(test_data[\"created\"].values, \"weekday\")\n",
    "hour_test_single = fmtTest(test_data[\"created\"].values, \"hour\")\n",
    "test_data.loc[:, \"text\"] = test_data[\"text\"].apply(rmvStops)\n",
    "test_data.loc[:, \"text\"] = test_data[\"text\"].apply(rmvLinks)\n",
    "\n",
    "#create binary matrix for weekdays for test\n",
    "n_test = day_test_single.shape[0]\n",
    "\n",
    "day_test = np.zeros((n_test, 7))\n",
    "for i in range(7):\n",
    "    day_test[:, i] = fmtTime(day_test_single, i)\n",
    "\n",
    "hour_test = np.zeros((n_test, 24))\n",
    "for i in range(24):\n",
    "    hour_test[:, i] = fmtTime(hour_test_single, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE: 0.8715596330275229\n"
     ]
    }
   ],
   "source": [
    "av = np.array([])\n",
    "numiters = 1\n",
    "\n",
    "for i in range(numiters):\n",
    "    #Split training and validation set\n",
    "    x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(data[\"text\"], data[\"label\"],test_size = .30)\n",
    "#     print (x_train)\n",
    "    tknzr = TweetTokenizer(strip_handles=False, reduce_len=False)\n",
    "    #Vectorize the data, create binary matrix of words\n",
    "    vectorizer = CountVectorizer(binary=True,lowercase=True,tokenizer=lambda text: tknzr.tokenize(text))\n",
    "#     print ()\n",
    "  \n",
    "    #TRAINING SET\n",
    "#     words_train = words_train.reshape(words_train.shape[0], 1)\n",
    "#     chars_train = chars_train.reshape(chars_train.shape[0], 1)\n",
    "    \n",
    "    x_train_vectors = vectorizer.fit_transform(x_train.values).toarray()\n",
    "#     for i, feature in enumerate(vectorizer.get_feature_names()):\n",
    "#         print(i, feature)\n",
    "#     print (x_train_vectors)\n",
    "#     x_train_vectors = np.append(x_train_vectors, day_train, 1)\n",
    "#     x_train_vectors = np.append(x_train_vectors, hour_train, 1)\n",
    "    \n",
    "    #VALIDATION SET\n",
    "#     words_valid = words_valid.reshape(words_valid.shape[0], 1)\n",
    "#     chars_valid = chars_valid.reshape(chars_valid.shape[0], 1) \n",
    "    \n",
    "    x_valid = x_valid.values\n",
    "    x_valid_transformed = vectorizer.transform(x_valid).toarray()\n",
    "    # validation set appended with day feature\n",
    "#     x_valid_transformed = np.append(x_valid_transformed,day_valid, 1)\n",
    "#     x_valid_transformed = np.append(x_valid_transformed,hour_valid, 1)\n",
    "    \n",
    "    #TEST SET\n",
    "    x_test = vectorizer.transform(test_data[\"text\"]).toarray()\n",
    "    # test set appended with day feature\n",
    "#     x_test = np.append(x_test, day_test, 1)\n",
    "#     x_test = np.append(x_test, hour_test ,1)\n",
    "\n",
    "    #Builds tree\n",
    "    numTrees = 150\n",
    "    dtclass = DecisionTreeClassifier(max_depth=3)\n",
    "    x = dtclass.fit(x_train_vectors, y_train.values)\n",
    "    \n",
    "    preds_test = dtclass.predict(x_test)\n",
    "    index = np.arange(0, len(preds_test))\n",
    "    df = pd.DataFrame(preds_test)\n",
    "    df.to_csv(\"plsworkdecision.csv\")\n",
    "    \n",
    "    preds = dtclass.predict(x_valid_transformed)\n",
    "    av = np.append(av, accuracy_score(preds, y_valid.values))\n",
    "\n",
    "# print(av)\n",
    "print(\"AVERAGE:\", np.sum(av)/numiters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE: 0.926605504587156\n"
     ]
    }
   ],
   "source": [
    "av = np.array([])\n",
    "numiters = 1\n",
    "\n",
    "for i in range(numiters):\n",
    "    #Split training and validation set\n",
    "#     print (len(day_created[0]))\n",
    "    x_train, x_valid, y_train, y_valid, day_train, day_valid, hour_train, hour_valid = sklearn.model_selection.train_test_split(data[\"text\"], data[\"label\"], day_created, hour_created, test_size = .2)\n",
    "#     use tweet tokenizer from nltk\n",
    "    tknzr = TweetTokenizer(strip_handles=False, reduce_len=False)\n",
    "    #Vectorize the data, create binary matrix of words\n",
    "    vectorizer = CountVectorizer(binary=True,lowercase=True,tokenizer=lambda text: tknzr.tokenize(text))\n",
    "    \n",
    "    #TRAINING SET\n",
    "#     words_train = words_train.reshape(words_train.shape[0], 1)\n",
    "#     chars_train = chars_train.reshape(chars_train.shape[0], 1)\n",
    "    \n",
    "    x_train_vectors = vectorizer.fit_transform(x_train.values).toarray()\n",
    "    x_train_vectors = np.append(x_train_vectors, day_train, 1)\n",
    "    x_train_vectors = np.append(x_train_vectors, hour_train, 1)\n",
    "    \n",
    "    #VALIDATION SET\n",
    "#     words_valid = words_valid.reshape(words_valid.shape[0], 1)\n",
    "#     chars_valid = chars_valid.reshape(chars_valid.shape[0], 1) \n",
    "    \n",
    "    x_valid = x_valid.values\n",
    "    x_valid_transformed = vectorizer.transform(x_valid).toarray()\n",
    "    # validation set appended with day feature\n",
    "    x_valid_transformed = np.append(x_valid_transformed,day_valid, 1)\n",
    "    x_valid_transformed = np.append(x_valid_transformed,hour_valid, 1)\n",
    "#     print (x_valid_transformed)\n",
    "    #TEST SET\n",
    "    x_test = vectorizer.transform(test_data[\"text\"]).toarray()\n",
    "    # test set appended with day feature\n",
    "    x_test = np.append(x_test, day_test, 1)\n",
    "    x_test = np.append(x_test, hour_test ,1)\n",
    "\n",
    "#     for i, feat in enumerate(vectorizer.get_feature_names()):\n",
    "#         print(i, feat)\n",
    "\n",
    "    linSVM = LinearSVC(dual=True,C=.5)\n",
    "    x = linSVM.fit(x_train_vectors, y_train.values)\n",
    "\n",
    "    preds_test = linSVM.predict(x_test)\n",
    "    index = np.arange(0, len(preds_test))\n",
    "    df = pd.DataFrame(preds_test)\n",
    "    df.to_csv(\"plsworksvm.csv\")\n",
    "    \n",
    "    preds = linSVM.predict(x_valid_transformed)\n",
    "    av = np.append(av, accuracy_score(preds, y_valid.values))\n",
    "#     for i in range(len(preds)):\n",
    "#         if preds[i] != y_valid.values[i]:\n",
    "#             print(str(y_valid.values[i]) + \"; \", x_valid[i])\n",
    "#             print(\"\\n\")\n",
    "\n",
    "# print(av)\n",
    "print(\"AVERAGE:\", np.sum(av)/numiters)\n",
    "# preds_test = dtclass.predict(x_test)\n",
    "# print(preds_test)\n",
    "#Validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE: 0.8486238532110092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "av = np.array([])\n",
    "numiters = 1\n",
    "\n",
    "for i in range(numiters):\n",
    "    #Split training and validation set\n",
    "    x_train, x_valid, y_train, y_valid, day_train, day_valid, hour_train, hour_valid = sklearn.model_selection.train_test_split(data[\"text\"], data[\"label\"], day_created, hour_created, test_size = .2)\n",
    "\n",
    "    #Vectorize the data, create tf-idf matrix of words\n",
    "    vectorizer = CountVectorizer(binary=True,lowercase=True)\n",
    "\n",
    "    #TRAINING SET\n",
    "    x_train_vectors = vectorizer.fit_transform(x_train.values).toarray()\n",
    "    \n",
    "    x_train_vectors = np.append(x_train_vectors, day_train, 1)\n",
    "    x_train_vectors = np.append(x_train_vectors, hour_train, 1)\n",
    "    \n",
    "    #VALIDATION SET\n",
    "    x_valid = x_valid.values\n",
    "    x_valid_transformed = vectorizer.transform(x_valid).toarray()\n",
    "    # validation set appended with day feature\n",
    "    x_valid_transformed = np.append(x_valid_transformed,day_valid, 1)\n",
    "    x_valid_transformed = np.append(x_valid_transformed,hour_valid, 1)\n",
    "    \n",
    "    #TEST SET\n",
    "    x_test = vectorizer.transform(test_data[\"text\"]).toarray()\n",
    "    # test set appended with day feature\n",
    "    x_test = np.append(x_test, day_test, 1)\n",
    "    x_test = np.append(x_test, hour_test ,1)\n",
    "\n",
    "#     for i, feat in enumerate(vectorizer.get_feature_names()):\n",
    "#         print(i, feat)\n",
    "\n",
    "    sgd = SGDClassifier()\n",
    "    x = sgd.fit(x_train_vectors, y_train.values)\n",
    "\n",
    "    preds_test = sgd.predict(x_test)\n",
    "    index = np.arange(0, len(preds_test))\n",
    "    df = pd.DataFrame(preds_test)\n",
    "    df.to_csv(\"plswork.csv\")\n",
    "    \n",
    "    preds = sgd.predict(x_valid_transformed)\n",
    "    av = np.append(av, accuracy_score(preds, y_valid.values))\n",
    "\n",
    "# print(av)\n",
    "print(\"AVERAGE:\", np.sum(av)/numiters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE: 0.8003058103975534\n"
     ]
    }
   ],
   "source": [
    "av = np.array([])\n",
    "numtimes = 15\n",
    "for i in range(numtimes):\n",
    "    #Split training and validation set\n",
    "    X_train, X_valid, Y_train, Y_valid = sklearn.model_selection.train_test_split(data[\"text\"], data[\"label\"] , test_size = .2)\n",
    "\n",
    "    #Vectorize the data, create tf-idf matrix of words\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    matrix = vectorizer.fit_transform(X_train.values)\n",
    "    matrix = matrix.toarray()\n",
    "\n",
    "    X_valid = X_valid.values\n",
    "    x_valid_transformed = vectorizer.transform(X_valid).toarray()\n",
    "    x_test = vectorizer.transform(test_data[\"text\"]).toarray()\n",
    "\n",
    "    #Builds tree\n",
    "\n",
    "    numTrees = 100\n",
    "#     dtclass = DecisionTreeClassifier(max_depth=2)\n",
    "    dtclass = AdaBoostClassifier(n_estimators=numTrees, base_estimator=DecisionTreeClassifier(max_depth=2))\n",
    "    x = dtclass.fit(matrix, Y_train.values)\n",
    "\n",
    "    \n",
    "    preds = dtclass.predict(x_valid_transformed)\n",
    "    av = np.append(av, accuracy_score(preds, Y_valid.values))\n",
    "print(\"AVERAGE:\", np.sum(av)/numtimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.arange(0, len(preds_test))\n",
    "df = pd.DataFrame(preds_test)\n",
    "df.to_csv(\"plswork.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ipykernel_py3]",
   "language": "python",
   "name": "Python [ipykernel_py3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
